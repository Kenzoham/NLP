{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbdb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referensi: https://www.linkedin.com/pulse/how-scrape-google-play-reviews-4-simple-steps-using-python-kundi/\n",
    "#download library google-play-scraper\n",
    "!pip install google-play-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e755c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import app\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape jumlah ulasan yang diinginkan\n",
    "from google_play_scraper import Sort, reviews\n",
    "\n",
    "result, continuation_token = reviews(\n",
    "    'com.shopee.id',\n",
    "    lang='id',  #disini kita mau men scrape data ulasan aplikasi shopee yang berada di google play store\n",
    "    country='id', #kita setting bahasa nya menjadi bahasa indonesia\n",
    "    sort=Sort.MOST_RELEVANT, # # kemudian kita gunakan most_relevan untuk mendapatkan ulasan yang paling relevant\n",
    "    count=1000, # disini jumlah ulasan yang mau kita ambil ada seribu\n",
    "    filter_score_with=None # # kemudian di filter_score kita gunakan None untuk mengambil semua score atau ratting bintang 1 sampai 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ee7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busu = pd.DataFrame(np.array(result),columns=['review'])\n",
    "\n",
    "df_busu = df_busu.join(pd.DataFrame(df_busu.pop('review').tolist()))\n",
    "\n",
    "df_busu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_busu.index) #kemudian hitung kembali berapa jumlah data yg didapatkan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d241d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_busu[['userName', 'score','at', 'content']].head()  #dari scrapping tsb didapatkan banyak sekali kolom, kemudian kolom\" tsb kita filter\n",
    "                                                        #sehingga didapatkan kolom username, score, at dan content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71576333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run This Code to Sort the Data By Date \n",
    "\n",
    "new_df = df_busu[['userName', 'score','at', 'content']]\n",
    "sorted_df = new_df.sort_values(by='at', ascending=False) #Sort by Newst, change to True if you want to sort by Oldest.\n",
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = sorted_df[['userName', 'score','at', 'content']] #kemudian kita simpan ke variabel my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df=my_df[['content', 'score']]#karena kita hanya membutuhkan kolom content dan score maka kita lakukan filter kolom lgi hingga menyisakan kolom content dan score. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3d9c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91830693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pelabelan(score):\n",
    "  if score < 3:\n",
    "    return 'Negatif'\n",
    "  elif score == 4 :\n",
    "    return 'Positif'\n",
    "  elif score == 5 :\n",
    "    return 'Positif'\n",
    "my_df['Label'] = my_df ['score'].apply(pelabelan)\n",
    "my_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c61e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv(\"scrapped_data.csv\", index = False)  #kemudian save menjadi file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43145670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "my_df = pd.read_csv('/content/scrapped_data.csv')\n",
    "my_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info() digunakan untuk menampilkan informasi detail tentang dataframe, \n",
    "#seperti jumlah baris data, nama-nama kolom berserta jumlah data dan tipe datanya, dan sebagainya.\n",
    "my_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tampilkan setiap baris yang memiliki nilai null (NaN) pada kolom apapun\n",
    "#Gunakan fitur isna() yang disediakan library pandas\n",
    "my_df.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbcd697",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca684b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mencari jumlah baris data yang bernilai null\n",
    "#terdapat kolom label memiliki nilai kosong\n",
    "my_df.isnull().sum()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a600e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.dropna(subset=['Label'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de279a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07df2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df.to_csv(\"shopeepreprocessing.csv\", index = False)  #simpan hasil file data cleaning dengan nama shopeepreprocessing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e561cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/shopeepreprocessing.csv')\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aecd82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def  clean_text(df, text_field, new_text_field_name):\n",
    "    my_df[new_text_field_name] = my_df[text_field].str.lower()\n",
    "    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
    "    # remove numbers\n",
    "    my_df[new_text_field_name] = my_df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem)) \n",
    "    return my_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a854a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df['text_clean'] = my_df['content'].str.lower()\n",
    "my_df['text_clean']\n",
    "data_clean = clean_text(my_df, 'content', 'text_clean')\n",
    "data_clean.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3baf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('indonesian')\n",
    "data_clean['text_StopWord'] = data_clean['text_clean'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop)]))\n",
    "data_clean.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2846c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data_clean['text_tokens'] = data_clean['text_StopWord'].apply(lambda x: word_tokenize(x))\n",
    "data_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7208a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06808fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ca0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------STEMMING -----------------\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "#import swifter\n",
    "\n",
    "\n",
    "# create stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemmed\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "term_dict = {}\n",
    "hitung=0\n",
    "\n",
    "for document in data_clean['text_tokens']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    hitung+=1\n",
    "    print(hitung,\":\",term,\":\" ,term_dict[term])\n",
    "\n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# apply stemmed term to dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]\n",
    "\n",
    "\n",
    "#script ini bisa dipisah dari eksekusinya setelah pembacaaan term selesai\n",
    "data_clean['text_steamindo'] = data_clean['text_tokens'].apply(lambda x:' '.join(get_stemmed_term(x)))\n",
    "data_clean.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0175ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean.to_csv('hasil_TextPreProcessing_shopee.csv', index= False) #kemudian simpan hasil text preprocessing ke file csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af615046",
   "metadata": {},
   "outputs": [],
   "source": [
    "#disini kita importkan library re, kemudian kita lakukan praproses\n",
    "import re\n",
    "def praproses(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)()(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2fce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#membagi data menjadi data training dan testing dengan test_size = 0.20 dan random state nya 0\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_clean['content'], data_clean['Label'], \n",
    "                                                    test_size = 0.20, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b0b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c176cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2198462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c68075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(tfidf_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c7716",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d1d42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(tfidf_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39729f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\", accuracy_score(y_test,predicted))\n",
    "print(\"MultinomialNB Precision:\", precision_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n",
    "print(\"MultinomialNB Recall:\", recall_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n",
    "print(\"MultinomialNB f1_score:\", f1_score(y_test,predicted, average=\"binary\", pos_label=\"Negatif\"))\n",
    "\n",
    "print(f'confusion_matrix:\\n {confusion_matrix(y_test, predicted)}')\n",
    "print('====================================================\\n')\n",
    "print(classification_report(y_test, predicted, zero_division=0))\n",
    "\n",
    "# Load dataset\n",
    "data_clean = pd.read_csv('hasil_TextPreProcessing_shopee.csv')\n",
    "\n",
    "#\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
